{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3017e0bc-1ac3-4ddf-bd0e-37cd04606cd3",
   "metadata": {},
   "source": [
    "## AVRO File to CSV (Code from Empatica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0603d823-e4e6-498a-a4f7-c68ba1c6e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avro.datafile import DataFileReader\n",
    "from avro.io import DatumReader\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4124e0f-f918-4944-b07b-c0403a576dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/katgm/Rutgers University/Michelle Chen - Rutgers_Neuropsych_Lab/COVID_Fatigue/RC_award/Data/Test_Data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f47ae-8e83-4d72-b7ad-cabe2c638479",
   "metadata": {},
   "source": [
    "## Export sensors data to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea88e24d-6643-4411-8830-3a7b6196f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(SubjID):\n",
    "    # Accelerometer\n",
    "    acc = data[\"rawData\"][\"accelerometer\"]\n",
    "    timestamp = [round(acc[\"timestampStart\"] + i * (1e6 / acc[\"samplingFrequency\"]))\n",
    "    \tfor i in range(len(acc[\"x\"]))]\n",
    "    # Convert ADC counts in g\n",
    "    delta_physical = acc[\"imuParams\"][\"physicalMax\"] - acc[\"imuParams\"][\"physicalMin\"]\n",
    "    delta_digital = acc[\"imuParams\"][\"digitalMax\"] - acc[\"imuParams\"][\"digitalMin\"]\n",
    "    x_g = [val * delta_physical / delta_digital for val in acc[\"x\"]]\n",
    "    y_g = [val * delta_physical / delta_digital for val in acc[\"y\"]]\n",
    "    z_g = [val * delta_physical / delta_digital for val in acc[\"z\"]]\n",
    "    with open(os.path.join(output_dir, SubjID + '_accelerometer.csv'), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"unix_timestamp\", \"x\", \"y\", \"z\"])\n",
    "        writer.writerows([[ts, x, y, z] for ts, x, y, z in zip(timestamp, x_g, y_g, z_g)])\n",
    "    \n",
    "    # Gyroscope\n",
    "    gyro = data[\"rawData\"][\"gyroscope\"]\n",
    "    timestamp = [round(gyro[\"timestampStart\"] + i * (1e6 / gyro[\"samplingFrequency\"]))\n",
    "        for i in range(len(gyro[\"x\"]))]\n",
    "    # Convert ADC counts in dps (degree per second)\n",
    "    delta_physical = gyro[\"imuParams\"][\"physicalMax\"] - gyro[\"imuParams\"][\"physicalMin\"]\n",
    "    delta_digital = gyro[\"imuParams\"][\"digitalMax\"] - gyro[\"imuParams\"][\"digitalMin\"]\n",
    "    x_dps = [val * delta_physical / delta_digital for val in gyro[\"x\"]]\n",
    "    y_dps = [val * delta_physical / delta_digital for val in gyro[\"y\"]]\n",
    "    z_dps = [val * delta_physical / delta_digital for val in gyro[\"z\"]]\n",
    "    with open(os.path.join(output_dir, SubjID + '_gyroscope.csv'), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"unix_timestamp\", \"x\", \"y\", \"z\"])\n",
    "        writer.writerows([[ts, x, y, z] for ts, x, y, z in zip(timestamp, x_dps, y_dps, z_dps)])\n",
    "    \n",
    "    # Eda\n",
    "    eda = data[\"rawData\"][\"eda\"]\n",
    "    timestamp = [round(eda[\"timestampStart\"] + i * (1e6 / eda[\"samplingFrequency\"]))\n",
    "        for i in range(len(eda[\"values\"]))]\n",
    "    with open(os.path.join(output_dir, SubjID + '_eda.csv'), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"unix_timestamp\", \"eda\"])\n",
    "        writer.writerows([[ts, eda] for ts, eda in zip(timestamp, eda[\"values\"])])\n",
    "    \n",
    "    # Temperature\n",
    "    tmp = data[\"rawData\"][\"temperature\"]\n",
    "    timestamp = [round(tmp[\"timestampStart\"] + i * (1e6 / tmp[\"samplingFrequency\"]))\n",
    "        for i in range(len(tmp[\"values\"]))]\n",
    "    with open(os.path.join(output_dir, SubjID + '_temperature.csv'), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"unix_timestamp\", \"temperature\"])\n",
    "        writer.writerows([[ts, tmp] for ts, tmp in zip(timestamp, tmp[\"values\"])])\n",
    "    \n",
    "    # Tags\n",
    "    tags = data[\"rawData\"][\"tags\"]\n",
    "    with open(os.path.join(output_dir, SubjID + '_tags.csv'), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"tags_timestamp\"])\n",
    "        writer.writerows([[tag] for tag in tags[\"tagsTimeMicros\"]])\n",
    "    \n",
    "    # BVP\n",
    "    bvp = data[\"rawData\"][\"bvp\"]\n",
    "    timestamp = [round(bvp[\"timestampStart\"] + i * (1e6 / bvp[\"samplingFrequency\"]))\n",
    "        for i in range(len(bvp[\"values\"]))]\n",
    "    with open(os.path.join(output_dir, SubjID + '_bvp.csv'), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"unix_timestamp\", \"bvp\"])\n",
    "        writer.writerows([[ts, bvp] for ts, bvp in zip(timestamp, bvp[\"values\"])])\n",
    "    \n",
    "    # Systolic peaks\n",
    "    sps = data[\"rawData\"][\"systolicPeaks\"]\n",
    "    with open(os.path.join(output_dir, SubjID + '_systolic_peaks.csv'), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"systolic_peak_timestamp\"])\n",
    "        writer.writerows([[sp] for sp in sps[\"peaksTimeNanos\"]])\n",
    "    \n",
    "    # Steps\n",
    "    steps = data[\"rawData\"][\"steps\"]\n",
    "    timestamp = [round(steps[\"timestampStart\"] + i * (1e6 / steps[\"samplingFrequency\"]))\n",
    "        for i in range(len(steps[\"values\"]))]\n",
    "    with open(os.path.join(output_dir, SubjID + '_steps.csv'), 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"unix_timestamp\", \"steps\"])\n",
    "        writer.writerows([[ts, step] for ts, step in zip(timestamp, steps[\"values\"])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e128187-b457-4781-8445-167f382a58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the location of the Avro file and output folder.\n",
    "dir = \"C:\\\\Users\\\\katgm\\\\Rutgers University\\\\Michelle Chen - Rutgers_Neuropsych_Lab\\\\COVID_Fatigue\\\\RC_award\\\\Data\\\\Test_Data\\\\\"\n",
    "input_dir = dir + \"YingfeiAvro\\\\\"\n",
    "output_dir = dir + \"YingfeiEmbracePlus\\\\\"\n",
    "\n",
    "counter = 0\n",
    "for filename in os.listdir(input_dir):\n",
    "    counter= counter+1\n",
    "    avro_file_path = os.path.join(input_dir, filename)\n",
    "    # checking if it is a file\n",
    "    reader = DataFileReader(open(avro_file_path, \"rb\"), DatumReader())\n",
    "    schema = json.loads(reader.meta.get('avro.schema').decode('utf-8'))\n",
    "    data= next(reader)\n",
    "    create_csv(\"Yingfei\" + str(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b72d7bd3-da35-4e96-9051-99a11feb91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining csv files into one\n",
    "\n",
    "def combine_csv(Subj, number):\n",
    "    bvp = pd.DataFrame()\n",
    "    for ii in range(number):\n",
    "        temp = pd.read_csv(output_dir + Subj + str(ii+1) + \"_bvp.csv\")\n",
    "        bvp = pd.concat([bvp, temp], ignore_index=True)\n",
    "    bvp[\"unix_timestamp\"] = bvp[\"unix_timestamp\"].map(lambda x: pd.to_datetime(x,unit=\"us\"))\n",
    "    bvp[\"unix_timestamp\"] = bvp[\"unix_timestamp\"].map(lambda x: x.round(freq=\"15.625ms\"))\n",
    "    \n",
    "    accelerometer = pd.DataFrame()\n",
    "    for ii in range(number):\n",
    "        temp = pd.read_csv(output_dir + Subj + str(ii+1) + \"_accelerometer.csv\")\n",
    "        accelerometer = pd.concat([accelerometer, temp], ignore_index=True)\n",
    "    accelerometer[\"unix_timestamp\"] = accelerometer[\"unix_timestamp\"].map(lambda x: pd.to_datetime(x,unit=\"us\"))\n",
    "    accelerometer[\"unix_timestamp\"] = accelerometer[\"unix_timestamp\"].map(lambda x: x.round(freq=\"15.625ms\"))\n",
    "    \n",
    "    eda = pd.DataFrame()\n",
    "    for ii in range(number):\n",
    "        temp = pd.read_csv(output_dir + Subj + str(ii+1) + \"_eda.csv\")\n",
    "        eda = pd.concat([eda, temp], ignore_index=True)\n",
    "    eda[\"unix_timestamp\"] = eda[\"unix_timestamp\"].map(lambda x: pd.to_datetime(x,unit=\"us\"))\n",
    "    eda[\"unix_timestamp\"] = eda[\"unix_timestamp\"].map(lambda x: x.round(freq=\"15.625ms\"))\n",
    "    \n",
    "    systolic_peaks = pd.DataFrame()\n",
    "    for ii in range(number):\n",
    "        temp = pd.read_csv(output_dir + Subj + str(ii+1) + \"_systolic_peaks.csv\")\n",
    "        systolic_peaks = pd.concat([systolic_peaks, temp], ignore_index=True)\n",
    "    systolic_peaks[\"systolic_peak_timestamp\"] = systolic_peaks[\"systolic_peak_timestamp\"].map(lambda x: pd.to_datetime(x,unit=\"ns\"))\n",
    "    systolic_peaks[\"systolic_peak_timestamp\"] = systolic_peaks[\"systolic_peak_timestamp\"].map(lambda x: x.round(freq=\"15.625ms\"))\n",
    "    \n",
    "    tags = pd.DataFrame()\n",
    "    for ii in range(number):\n",
    "        temp = pd.read_csv(output_dir + Subj + str(ii+1) + \"_tags.csv\")\n",
    "        tags = pd.concat([tags, temp], ignore_index=True)\n",
    "    tags[\"tags_timestamp\"] = tags[\"tags_timestamp\"].map(lambda x: pd.to_datetime(x,unit=\"us\"))\n",
    "    tags[\"tags_timestamp\"] = tags[\"tags_timestamp\"].map(lambda x: x.round(freq=\"15.625ms\"))\n",
    "    \n",
    "    temperature = pd.DataFrame()\n",
    "    for ii in range(number):\n",
    "        temp = pd.read_csv(output_dir + Subj + str(ii+1) + \"_temperature.csv\")\n",
    "        temperature = pd.concat([temperature, temp], ignore_index=True)\n",
    "    temperature[\"unix_timestamp\"] = temperature[\"unix_timestamp\"].map(lambda x: pd.to_datetime(x,unit=\"us\"))\n",
    "    temperature[\"unix_timestamp\"] = temperature[\"unix_timestamp\"].map(lambda x: x.round(freq=\"15.625ms\"))\n",
    "    \n",
    "    #gyroscope is empty\n",
    "    # KatTest_gyroscope = pd.DataFrame()\n",
    "    # for ii in range(6):\n",
    "    #     temp = pd.read_csv(output_dir + \"KatTest\" + str(ii+1) + \"_gyroscope.csv\")\n",
    "    #     KatTest_gyroscope = pd.concat([KatTest_gyroscope, temp], ignore_index=True)\n",
    "    # KatTest_accelerometer[\"unix_timestamp\"] = KatTest_accelerometer[\"unix_timestamp\"].map(lambda x: pd.to_datetime(x,unit=\"us\"))\n",
    "    \n",
    "    # steps not really relevant for comparison data\n",
    "    # KatTest_steps = pd.DataFrame()\n",
    "    # for ii in range(6):\n",
    "    #     temp = pd.read_csv(output_dir + \"KatTest\" + str(ii+1) + \"_steps.csv\")\n",
    "    #     KatTest_steps = pd.concat([KatTest_steps, temp], ignore_index=True)\n",
    "\n",
    "    merged = pd.merge(bvp, accelerometer, on = \"unix_timestamp\", how=\"left\").merge(eda, on=\"unix_timestamp\",how=\"left\").merge(temperature, on=\"unix_timestamp\",how=\"left\")\n",
    "    merged = merged.rename(columns={\"unix_timestamp\": \"timestamp\", \"x\": \"acc_x\", \"y\":\"acc_y\", \"z\":\"acc_z\"})\n",
    "\n",
    "    os.chdir(output_dir)\n",
    "    \n",
    "    merged.to_csv(Subj + '.csv', index=False)\n",
    "    tags.to_csv(Subj + \"_tags.csv\", index=False)\n",
    "    systolic_peaks.to_csv(Subj + \"_peaks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d1f73c2-3b53-48d6-aebe-8e728aaf6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_csv(\"Yingfei\",2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f58aea5a-4d85-45e0-8bf3-9abba5260980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the timestamps don't match up between bvp, accelerometer, eda, etc. Check to see what the sampling frequency actually is.\n",
    "#are the observations actually evenly spaced apart?\n",
    "\n",
    "bvp_diff = KatTest_bvp[\"unix_timestamp\"].diff() \n",
    "#they are evenly spaced. 0.015625, which is 1/64 of a second. As expected. 64Hz\n",
    "acc_diff = KatTest_accelerometer[\"unix_timestamp\"].diff()\n",
    "#also evenly spaced at 1/64 of a second, sometimes one or two seconds off. 64Hz\n",
    "#they just have different starting microseconds.\n",
    "eda_diff = KatTest_eda[\"unix_timestamp\"].diff()\n",
    "#its about 0.25, which is 1/4 of a second as expected. 4Hz\n",
    "systolic_diff = KatTest_systolic_peaks[\"systolic_peak_timestamp\"].diff()\n",
    "#all different since it's peaks\n",
    "tags_diff = KatTest_tags[\"tags_timestamp\"].diff() #about 5 min apart since that's when i tagged\n",
    "temp_diff = KatTest_temperature[\"unix_timestamp\"].diff()\n",
    "#very close to 1 second apart. 1 Hz same as e4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62d031-103b-4cea-a897-49ffb8b3b3f1",
   "metadata": {},
   "source": [
    "# Organizing data to mirror the E4 dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56e4d52c-1ec2-4212-86d7-1d43d2017e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "KTest = pd.read_csv(dir + \"KatEmbracePlus2/Kat.csv\")\n",
    "KTags = pd.read_csv(dir + \"KatEmbracePlus2/Kat_tags.csv\")\n",
    "KTest['timestamp'] = pd.to_datetime(KTest['timestamp'])\n",
    "KTags['tags_timestamp'] = pd.to_datetime(KTags['tags_timestamp'])\n",
    "\n",
    "DTest = pd.read_csv(dir + \"DarsheeEmbracePlus2/Darshee.csv\")\n",
    "DTags = pd.read_csv(dir + \"DarsheeEmbracePlus2/Darshee_tags.csv\")\n",
    "DTags['tags_timestamp'] = pd.to_datetime(DTags['tags_timestamp'])\n",
    "DTest['timestamp'] = pd.to_datetime(DTest['timestamp'])\n",
    "\n",
    "YTest = pd.read_csv(dir + \"YingfeiEmbracePlus/Yingfei.csv\")\n",
    "YTags = pd.read_csv(dir + \"YingfeiEmbracePlus/Yingfei_tags.csv\")\n",
    "YTags['tags_timestamp'] = pd.to_datetime(YTags['tags_timestamp'])\n",
    "YTest['timestamp'] = pd.to_datetime(YTest['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2759a119-46df-4a80-b328-793337411de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating tags\n",
    "#start at tag 2 and increment in 5 minute intervals\n",
    "#darshee's tags\n",
    "DInit = DTags.iloc[2,0]\n",
    "DTags2 = pd.DataFrame({\"tags_timestamp\":[DInit,DInit+pd.Timedelta(minutes=5),DInit+pd.Timedelta(minutes=10),DInit+pd.Timedelta(minutes=15),\n",
    "                                          DInit+pd.Timedelta(minutes=20),DInit+pd.Timedelta(minutes=25),DInit+pd.Timedelta(minutes=30),\n",
    "                                          DInit+pd.Timedelta(minutes=35)]})\n",
    "#generating yingfei's\n",
    "YInit = YTags.iloc[0,0]\n",
    "YTags2 = pd.DataFrame({\"tags_timestamp\":[YInit,YInit+pd.Timedelta(minutes=5),YInit+pd.Timedelta(minutes=10),YInit+pd.Timedelta(minutes=15),\n",
    "                                          YInit+pd.Timedelta(minutes=20),YInit+pd.Timedelta(minutes=25),YInit+pd.Timedelta(minutes=30),\n",
    "                                          YInit+pd.Timedelta(minutes=35)]})\n",
    "\n",
    "#generating kat's\n",
    "KInit = KTags.iloc[1,0]\n",
    "KTags2 = pd.DataFrame({\"tags_timestamp\":[KInit,KInit+pd.Timedelta(minutes=5),KInit+pd.Timedelta(minutes=10),KInit+pd.Timedelta(minutes=15),\n",
    "                                          KInit+pd.Timedelta(minutes=20),KInit+pd.Timedelta(minutes=25),KInit+pd.Timedelta(minutes=30),\n",
    "                                          KInit+pd.Timedelta(minutes=35)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5a4c5e5-f398-472a-8db5-813c993ab6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_blocks(tags, data):    \n",
    "    if len(tags) != 8:\n",
    "        print(\"Incorrect number of tags. There are \" + str(len(tags)) + \" when there should be 8.\")\n",
    "        return None\n",
    "\n",
    "     \n",
    "    # remove data before and after tags range\n",
    "    remove = data[(data['timestamp'] > tags.iloc[7,0])].index\n",
    "    remove2 = data[(data['timestamp'] < tags.iloc[0,0])].index\n",
    "    temp = data.drop(remove)\n",
    "    df = temp.drop(remove2)\n",
    "    # Add block numbers to dataframe\n",
    "    conditions = [\n",
    "        (df['timestamp'] >= tags.iloc[0,0]) & (df['timestamp'] < tags.iloc[1,0]),\n",
    "        (df['timestamp'] >= tags.iloc[1,0]) & (df['timestamp'] < tags.iloc[2,0]),\n",
    "        (df['timestamp'] >= tags.iloc[2,0]) & (df['timestamp'] < tags.iloc[3,0]),\n",
    "        (df['timestamp'] >= tags.iloc[3,0]) & (df['timestamp'] < tags.iloc[4,0]),\n",
    "        (df['timestamp'] >= tags.iloc[4,0]) & (df['timestamp'] < tags.iloc[5,0]),\n",
    "        (df['timestamp'] >= tags.iloc[5,0]) & (df['timestamp'] < tags.iloc[6,0]),\n",
    "        (df['timestamp'] >= tags.iloc[6,0]) & (df['timestamp'] < tags.iloc[7,0]),\n",
    "    ]\n",
    "\n",
    "    blocks = [0,1,2,3,4,5,6]\n",
    "    #fakeRating = [43,55,56,55,60,69,70]\n",
    "    \n",
    "    df['Block'] = np.select(conditions, blocks, default=pd.NA)\n",
    "    #df['Fatigue_Rating'] = np.select(conditions, fakeRating, default=pd.NA)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b06cbaa-bbb6-4e31-b628-07647a081da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "KatFinal2 = add_blocks(KTags2,KTest)\n",
    "YingfeiFinal2 = add_blocks(YTags2,YTest)\n",
    "DarsheeFinal2 = add_blocks(DTags2,DTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "713f8e28-3867-4b80-b75f-61be7e4b09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "KatFinal2.to_csv(\"KatEmP2.csv\",index=False)\n",
    "YingfeiFinal2.to_csv(\"YingfeiEmP2.csv\",index=False)\n",
    "DarsheeFinal2.to_csv(\"DarsheeEmP2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643bc68e-9b56-480c-a329-0816203542b0",
   "metadata": {},
   "source": [
    "# Reading E4 Files in (predominantly using code from lab_data_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8c46576a-4326-490f-b934-40ef299f5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a00858c-3071-46b3-91bc-b841fc80430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ef14288d-3c67-41e7-b0c3-41b628d790ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_lab_data(SubjID):\n",
    "    wd = os.getcwd() + '\\\\' + SubjID + 'E42\\\\'\n",
    "    eda = pd.read_csv(wd + 'EDA.csv', header=None).to_numpy().flatten()\n",
    "    bvp = pd.read_csv(wd + 'BVP.csv', header=None).to_numpy().flatten()\n",
    "    acc = pd.read_csv(wd + 'ACC.csv', header=None).to_numpy()\n",
    "    temp = pd.read_csv(wd + 'TEMP.csv', header=None).to_numpy().flatten()\n",
    "    \n",
    "    init_time = pd.to_datetime(eda[0],unit=\"s\") #they all have the same initial time\n",
    "    \n",
    "    eda = eda[2:]\n",
    "    bvp = bvp[2:]\n",
    "    acc = acc[2:]\n",
    "    temp = temp[2:]\n",
    "    \n",
    "    eda_interval = timedelta(seconds=1/4)\n",
    "    bvp_interval = timedelta(seconds=1/64)\n",
    "    acc_interval = timedelta(seconds=1/32)\n",
    "    temp_interval = timedelta(seconds=1/4)\n",
    "    \n",
    "    eda_timestamps = [init_time + i * eda_interval for i in range(len(eda))]\n",
    "    bvp_timestamps = [init_time + i * bvp_interval for i in range(len(bvp))]\n",
    "    acc_timestamps = [init_time + i * acc_interval for i in range(len(acc))]\n",
    "    temp_timestamps = [init_time + i * temp_interval for i in range(len(temp))]\n",
    "    \n",
    "    # Create a new DataFrame with the timestamps and the original data columns\n",
    "    eda_df = pd.DataFrame(data = eda, columns=['eda'])\n",
    "    eda_df['timestamp'] = eda_timestamps\n",
    "\n",
    "    bvp_df = pd.DataFrame(data = bvp, columns=['bvp'])\n",
    "    bvp_df['timestamp'] = bvp_timestamps\n",
    "\n",
    "    acc_df = pd.DataFrame(data = acc, columns=['acc_x', 'acc_y', 'acc_z'])\n",
    "    acc_df['timestamp'] = acc_timestamps\n",
    "\n",
    "    temp_df = pd.DataFrame(data = temp, columns=['temperature'])\n",
    "    temp_df['timestamp'] = temp_timestamps\n",
    " \n",
    "    #merge dataframes together, change column order, LEAVING NA values, because we should filter the signal first!\n",
    "    merged_df = pd.merge(bvp_df, eda_df, on='timestamp', how='left').merge(acc_df, on='timestamp', how='left').merge(temp_df, on='timestamp', how='left')\n",
    "    \n",
    "    new_cols = ['timestamp','bvp', 'acc_x','acc_y','acc_z','eda','temperature']\n",
    "    \n",
    "    merged_df = merged_df[new_cols]\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4de88a96-abbd-448a-b970-61f6a330b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "YingfeiE42 = concatenate_lab_data(\"Yingfei\")\n",
    "KatE42 = concatenate_lab_data(\"Kat\")\n",
    "DarE42 = concatenate_lab_data(\"Darshee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "adc020cc-5fc5-4670-8894-60ec598ba820",
   "metadata": {},
   "outputs": [],
   "source": [
    "DarE42Final = add_blocks(DTags2, DarE42)\n",
    "YingfeiE42Final = add_blocks(YTags2, YingfeiE42)\n",
    "KatE42Final = add_blocks(KTags2, KatE42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "beed07e3-1ef2-49cd-81aa-d807d0fe958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dir + \"FinalData\")\n",
    "KatE42Final.to_csv(\"KatE42.csv\",index=False)\n",
    "YingfeiE42Final.to_csv(\"YingfeiE42.csv\",index=False)\n",
    "DarE42Final.to_csv(\"DarE42.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
